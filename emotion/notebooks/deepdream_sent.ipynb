{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deepdream refactored to work with my models and code\n",
    "import os\n",
    "# Set path to root directory so we can access core, tasks, data, etc.\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from core.utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_model(sess, params):\n",
    "    \"\"\"Load model from checkpoint\"\"\"\n",
    "    # First load graph\n",
    "    # .meta file defines graph - they should all be the same? So just take any one\n",
    "    # meta_file = [f for f in os.listdir(params['ckpt_dirpath']) if f.endswith('meta')][0]\n",
    "    # meta_filepath = os.path.join(params['ckpt_dirpath'], meta_file)\n",
    "    # saver = tf.train.import_meta_graph(meta_filepath)\n",
    "\n",
    "    # Load weights\n",
    "    saver = None\n",
    "#     saver = tf.train.Saver(tf.global_variables())\n",
    "    if params['load_epoch'] is not None:        # load the checkpoint for the given epoch\n",
    "        ckpt_name = _get_ckpt_basename(params) + '-' + str(params['load_epoch'])\n",
    "        ckpt_path = os.path.join(params['ckpt_dirpath'], ckpt_name)\n",
    "        # saver.restore(sess, os.path.join(params['ckpt_dirpath'], ckpt_name))\n",
    "    else:       # 'checkpoint' binary file keeps track of latest checkpoints. Use it to to get most recent\n",
    "        ckpt = tf.train.get_checkpoint_state(params['ckpt_dirpath'])\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        ckpt_path = os.path.join(params['ckpt_dirpath'], ckpt_name)\n",
    "#     saver.restore(sess, ckpt_path)\n",
    "\n",
    "    return saver, ckpt_path\n",
    "\n",
    "def freeze_graph(ckpt_dirpath, arch, obj, load_epoch=None):\n",
    "    params = {'ckpt_dirpath': ckpt_dirpath, 'arch': arch, 'obj': obj, 'load_epoch': load_epoch}\n",
    "\n",
    "    saver, ckpt_path = load_model(None, params)\n",
    "    input_checkpoint = ckpt_path\n",
    "    \n",
    "    # We retrieve our checkpoint fullpath\n",
    "    # checkpoint = tf.train.get_checkpoint_state(model_folder)\n",
    "    # input_checkpoint = checkpoint.model_checkpoint_path\n",
    "    #\n",
    "    # # We precise the file fullname of our freezed graph\n",
    "    # absolute_model_folder = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "#     output_graph = absolute_model_folder + \"/frozen_model.pb\"\n",
    "    output_graph = os.path.join(ckpt_dirpath, \"frozen_model.pb\")\n",
    "\n",
    "    #\n",
    "#     Before exporting our graph, we need to precise what is our output node\n",
    "#     This is how TF decides what part of the Graph he has to keep and what part it can dump\n",
    "#     NOTE: this variables is plural, because you can have multiple output nodes\n",
    "    # output_node_names = \"Accuracy/predictions\"\n",
    "    output_node_names = \"img_batch,fc/4_w,fc/4_b\"\n",
    "    \n",
    "    # We clear the devices, to allow TensorFlow to control on the loading where it wants operations to be calculated\n",
    "    clear_devices = True\n",
    "    \n",
    "    # We import the meta graph and retrive a Saver\n",
    "    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "    # We retrieve the protobuf graph definition\n",
    "    graph = tf.get_default_graph()\n",
    "    input_graph_def = graph.as_graph_def()\n",
    "\n",
    "    # We start a session and restore the graph weights\n",
    "    with tf.Session() as sess:\n",
    "        print input_checkpoint\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "\n",
    "        # We use a built-in TF helper to export variables to constant\n",
    "        output_graph_def = graph_util.convert_variables_to_constants(\n",
    "            sess, # The session is used to retrieve the weights\n",
    "            input_graph_def, # The graph_def is used to retrieve the nodes\n",
    "            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n",
    "        )\n",
    "\n",
    "        # Finally we serialize and dump the output graph to the filesystem\n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
    "        \n",
    "def _get_ckpt_basename(params):\n",
    "    \"\"\"Use to save and load\"\"\"\n",
    "    return params['arch'] + '-' + params['obj']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasks/image_sent/checkpoints/2016-12-26___08-45-18/basic_cnn-sent_biclass-4\n",
      "INFO:tensorflow:Froze 24 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 24 variables.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "img_batch:0 is not in graph",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-70f9191b5c5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfreeze_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tasks/image_sent/checkpoints/2016-12-26___08-45-18/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'basic_cnn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sent_biclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-27214321b1f9>\u001b[0m in \u001b[0;36mfreeze_graph\u001b[0;34m(ckpt_dirpath, arch, obj, load_epoch)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# The session is used to retrieve the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0minput_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# The graph_def is used to retrieve the nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0moutput_node_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The output node names are used to select the usefull nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         )\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.pyc\u001b[0m in \u001b[0;36mconvert_variables_to_constants\u001b[0;34m(sess, input_graph_def, output_node_names, variable_names_whitelist)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;31m# This graph only includes the nodes needed to evaluate the output nodes, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0;31m# removes unneeded nodes like those involved in saving and assignment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m   \u001b[0minference_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_sub_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_node_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0moutput_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.pyc\u001b[0m in \u001b[0;36mextract_sub_graph\u001b[0;34m(graph_def, dest_nodes)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdest_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname_to_node_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%s is not in graph\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   \u001b[0mnodes_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: img_batch:0 is not in graph"
     ]
    }
   ],
   "source": [
    "freeze_graph('tasks/image_sent/checkpoints/2016-12-26___08-45-18/', 'basic_cnn', 'sent_biclass', load_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempted to map inputs that were not found in graph_def: [img_batch:0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c690a44ed5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mimagenet_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m117.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mt_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_input\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mimagenet_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'img_batch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt_preprocessed\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.pyc\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    437\u001b[0m       raise ValueError(\n\u001b[1;32m    438\u001b[0m           \u001b[0;34m'Attempted to map inputs that were not found in graph_def: [%s]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m           % ', '.join(unused_input_keys))\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_elements\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempted to map inputs that were not found in graph_def: [img_batch:0]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model_fn = 'tasks/image_sent/checkpoints/2016-12-26___08-45-18/frozen_model.pb'\n",
    "\n",
    "\n",
    "# creating TensorFlow session and loading the model\n",
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "with tf.gfile.FastGFile(model_fn, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "t_input = tf.placeholder(np.float32, name='img_batch') # define the input tensor\n",
    "imagenet_mean = 117.0\n",
    "t_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)\n",
    "tf.import_graph_def(graph_def, {'img_batch':t_preprocessed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
